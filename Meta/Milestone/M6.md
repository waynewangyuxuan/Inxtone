# Milestone 6: Smart Intake

- **Status**: Complete ✅
- **Target Date**: TBD (after M5)
- **Owner**: Wayne
- **Duration**: 2.5 weeks
- **Dependencies**: M4.5 (Writing Intelligence)

## Goal

Let writers populate Inxtone's Story Bible from natural language, existing chapters, and outlines — instead of field-by-field form entry. The form becomes the review interface, not the input interface. This is the key adoption unlock: without Smart Intake, Inxtone is demo-ready but not adoption-ready.

## Success Criteria

```
✓ Writer can paste natural language description → structured Story Bible entities created
✓ Writer can import existing chapters → characters, relationships, world, plot auto-extracted
✓ All intake flows follow "AI suggests → human reviews → confirm" pattern
✓ Intake produces valid Story Bible data that passes existing consistency rules
```

## Scope

### In Scope

1. **Natural Language → Story Bible (P0)**
   - Text input panel: paste or type free-form descriptions
   - AI decomposition pipeline: text → structured entities (character, relationship, world rule, location, faction, etc.)
   - Review UI: present extracted entities for confirmation/edit before committing
   - Batch confirmation: accept all / accept individually / edit and accept
   - Works for all Story Bible entity types

2. **Chapter Import + Auto-Extraction (P0)**
   - File upload: accept .txt, .md, .docx chapter files (single or bulk)
   - Paste mode: paste chapter text directly
   - AI extraction pipeline: read all chapters → extract characters, relationships, world rules, locations, factions, foreshadowing, plot threads
   - Step-by-step review: characters first → relationships → world → plot (not all at once)
   - Merge logic: detect duplicates when importing into existing Story Bible
   - Chapter content stored in WritingService after import

### Out of Scope

- **Outline Import → Plot Architecture** — deferred to post-MVP (M9+)
- Ambient detection during writing (auto-detect new entities as writer types) — M9+
- User-definable Story Bible schema (custom entity types/fields) — M9+
- Import from Scrivener, Notion, or other writing tools — future
- Real-time collaborative import — future

## Deliverables

| # | Deliverable | Acceptance Criteria |
|---|-------------|---------------------|
| 1 | **AI Decomposition Pipeline** | Natural language → structured entities, all 9 Bible domains supported |
| 2 | **Intake API Endpoints** | `POST /api/intake/decompose`, `POST /api/intake/import-chapters`, `POST /api/intake/commit` |
| 3 | **Review & Confirm UI** | Step-by-step entity review, edit-before-commit, batch accept |
| 4 | **Chapter Import** | Upload/paste chapters → full Story Bible extraction → review → commit |
| 5 | **Duplicate Detection** | Importing into existing Bible detects and handles entity conflicts |

---

## Tasks

### Phase 1: AI Decomposition Pipeline (Day 1-5)

Core AI infrastructure shared by all intake modes.

- [ ] Design `IntakeService` interface
  - [ ] `decompose(text: string, hint?: EntityType): DecomposedResult`
  - [ ] `extractFromChapters(chapters: ChapterText[]): ExtractionResult`
- [ ] Implement decomposition prompt templates
  - [ ] Character extraction prompt (→ name, role, motivation layers, voice, appearance)
  - [ ] Relationship extraction prompt (→ character pairs, type, dynamics, R1 fields)
  - [ ] World extraction prompt (→ power system, social rules, constraints)
  - [ ] Location extraction prompt (→ name, type, atmosphere, significance)
  - [ ] Faction extraction prompt (→ name, type, goals, resources, conflicts)
  - [ ] Foreshadowing extraction prompt (→ content, type, planted context, term)
  - [ ] Arc extraction prompt (→ main/sub arcs, sections, progression)
- [ ] Implement `IntakeService.decompose()`
  - [ ] Send text + prompt to Gemini 3.0 Pro
  - [ ] Parse structured JSON response
  - [ ] Validate against existing Story Bible schemas (Zod)
  - [ ] Return typed `DecomposedResult` with confidence scores
- [ ] Implement response parsing with fallback
  - [ ] JSON mode when available
  - [ ] Regex/heuristic fallback for malformed responses
  - [ ] Partial success handling (some entities parsed, others failed)
- [ ] Tests: decomposition prompts produce valid schema-conforming output

### Phase 2: Natural Language → Story Bible UI (Day 4-8)

- [ ] Create `IntakePanel.tsx` — main intake interface
  - [ ] Large text input area (paste / type)
  - [ ] Entity type hint selector (optional: "I'm describing a character" / "This is world-building" / auto-detect)
  - [ ] "Decompose" button → loading state → results
- [ ] Create `IntakeReviewPanel.tsx` — review extracted entities
  - [ ] Entity cards grouped by type (characters, relationships, world, etc.)
  - [ ] Each card: preview of extracted data, "Accept" / "Edit" / "Reject"
  - [ ] "Edit" opens pre-filled Story Bible form (existing form components)
  - [ ] Batch actions: "Accept All", "Accept All Characters", etc.
  - [ ] Confidence indicator per entity (high/medium/low extraction confidence)
- [ ] Create `POST /api/intake/decompose` route
  - [ ] Accept `{ text, hint?, existingEntityIds? }`
  - [ ] Return `{ entities: DecomposedEntity[], warnings: string[] }`
- [ ] Create `POST /api/intake/commit` route
  - [ ] Accept `{ entities: ConfirmedEntity[] }`
  - [ ] Write confirmed entities to Story Bible repos in transaction
  - [ ] Return created entity IDs
- [ ] Integrate into Web UI navigation (sidebar entry point)
- [ ] Tests: full flow from text → decompose → review → commit → verify in DB

### Phase 3: Chapter Import + Auto-Extraction (Day 7-14)

- [ ] Create `ChapterImportPanel.tsx`
  - [ ] File upload zone (drag & drop, .txt / .md / .docx)
  - [ ] Paste mode (large textarea for direct paste)
  - [ ] Chapter boundary detection (auto-split by "Chapter N" / "第N章" patterns)
  - [ ] Manual boundary adjustment if auto-detect fails
  - [ ] Preview: list of detected chapters with word counts
- [ ] Implement `IntakeService.extractFromChapters()`
  - [ ] Chunking strategy: process chapters in batches (context window budget)
  - [ ] Multi-pass extraction: characters first → relationships → world → plot
  - [ ] Cross-chapter entity resolution (same character mentioned different ways)
  - [ ] Foreshadowing detection (plants + hints across chapters)
- [ ] Create step-by-step review wizard
  - [ ] Step 1: Characters — extracted character cards, confirm/edit/reject
  - [ ] Step 2: Relationships — extracted relationships between confirmed characters
  - [ ] Step 3: World — power system, rules, locations, factions
  - [ ] Step 4: Plot — arcs, foreshadowing, hooks
  - [ ] Progress indicator, back/forward navigation
  - [ ] Each step only shows entities not yet confirmed
- [ ] Implement duplicate detection
  - [ ] Name matching (fuzzy: "Chen Wei" = "陈伟" = "Senior Brother Chen")
  - [ ] AI-assisted: "Is this the same character?" prompt for ambiguous cases
  - [ ] Merge UI: show existing entity vs. imported entity, let user choose
- [ ] Chapter content import
  - [ ] Create chapters in WritingService with imported content
  - [ ] Link chapter ↔ extracted characters, locations
  - [ ] Populate chapter outlines from extracted structure
- [ ] Create `POST /api/intake/import-chapters` route
  - [ ] Accept `{ chapters: { title, content }[], options? }`
  - [ ] Return `{ extraction: ExtractionResult, chapterIds: string[] }`
- [ ] Tests: import sample chapters → verify complete Story Bible generated

### Phase 4: Integration + Polish (Day 13-17)

- [ ] Unified intake entry point
  - [ ] "Import" button in sidebar → modal with 2 tabs (Text / Chapters)
  - [ ] Dashboard empty state includes intake CTA
  - [ ] Welcome screen adds "Import Existing Work" option
- [ ] Error handling
  - [ ] AI extraction failure → graceful fallback ("couldn't parse, try shorter text")
  - [ ] Partial extraction → show what succeeded, let user retry rest
  - [ ] Network/API errors → retry with backoff
- [ ] Performance
  - [ ] Progress indicator for long extractions (chapter import may take minutes)
  - [ ] Streaming status updates via SSE ("Analyzing chapter 3 of 50...")
  - [ ] Cancel button for long operations
- [ ] E2E test: empty project → import 10 chapters → review all entities → full Story Bible populated
- [ ] E2E test: existing project → import new chapters → duplicate detection → merge
- [ ] Update Progress.md, CHANGELOG.md

---

## Test Plan

### Unit Tests
- [ ] Decomposition prompt templates produce valid JSON for each entity type
- [ ] Schema validation catches malformed AI responses
- [ ] Duplicate detection: exact match, fuzzy match, no-match cases
- [ ] Chapter boundary detection: Chinese patterns, English patterns, mixed

### Integration Tests
- [ ] Full decompose pipeline: text → AI → parse → validate → return
- [ ] Full chapter import pipeline: upload → chunk → extract → review → commit
- [ ] Commit transaction: all entities written atomically, rollback on failure

### E2E Tests
- [ ] Empty project + natural language intake → Story Bible populated
- [ ] Empty project + chapter import (10 chapters) → complete Story Bible + chapters
- [ ] Existing project + chapter import → duplicate detection + merge
- [ ] Import + then write with AI → context engine picks up imported entities

---

## Technical Notes

### AI Decomposition Architecture

```
User Input (text / chapters)
         │
         v
   IntakeService
         │
         ├─ Prompt Template (entity-type-specific)
         │
         ├─ Gemini 3.0 Pro (structured output)
         │
         ├─ Response Parser (JSON → typed entities)
         │
         ├─ Schema Validator (Zod — same schemas as repos)
         │
         v
   DecomposedResult { entities, confidence, warnings }
         │
         v
   Review UI (human confirms)
         │
         v
   Commit (repos write in transaction)
```

### Chunking Strategy for Chapter Import

```
Total chapters: N
Context window: ~1M tokens (Gemini 3.0 Pro)

Strategy:
- First pass: send ALL chapters as read-only context
  → extract character list, relationship list, world rules
  (Gemini 3.0 Pro's 1M context can handle ~500K words)

- Second pass (if needed): per-arc detailed extraction
  → foreshadowing, hooks, pacing within arc boundaries

- Entity resolution: cross-reference extracted entities across passes
```

### Duplicate Detection Approach

```typescript
interface DuplicateCandidate {
  imported: StoryBibleEntity;
  existing: StoryBibleEntity;
  confidence: number; // 0-1
  reason: string;     // "exact name match" | "AI similarity" | "alias match"
}

// Tiered matching:
// 1. Exact name match → confidence 0.95+
// 2. Alias/nickname match → confidence 0.7-0.9
// 3. AI-assisted ("same entity?") → confidence varies
// 4. No match → create new
```

---

## Dependencies

- M4.5 (Writing Intelligence) — entity extraction patterns established
- Gemini 3.0 Pro API — structured output / JSON mode preferred
- Existing Story Bible repos — write targets for committed entities
- Existing form components — reused in review/edit UI

## Risks

| Risk | Likelihood | Impact | Mitigation |
|------|------------|--------|------------|
| AI extraction quality inconsistent | High | High | "AI suggests, human confirms" UX — AI doesn't need to be perfect, just faster than manual |
| Chapter import takes too long (minutes) | Medium | Medium | Streaming progress updates, chunked processing, cancel support |
| Duplicate detection false positives | Medium | Medium | Always show both entities side-by-side, let user decide |
| Scope creep across 2 intake modes | Medium | Medium | P0 = natural language + chapter import. Outline import deferred to post-MVP |
| Structured output format varies | Medium | Medium | Zod validation + fallback parsing, retry on failure |

---

## Notes

- Natural language intake 是最小可行 intake — 即使 chapter import 延期，用户至少可以粘贴描述来快速建库
- Chapter import 是最高价值 intake — 已有作品的作者是最可能的早期用户
- Review UI 复用现有 Story Bible 表单组件，不需要从零设计
- Confidence score 帮助用户决定哪些需要仔细审查，哪些可以直接 accept
- 后续 M9+ 可以在此基础上加 ambient detection（写作过程中自动发现新实体）
- Outline Import 推迟到 post-MVP — NL intake + chapter import 已经覆盖核心场景
